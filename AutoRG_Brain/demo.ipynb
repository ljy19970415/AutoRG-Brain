{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference.inferenceSdk import SegModel\n",
    "from network_training.model_restore import load_model_and_checkpoint_files_llm\n",
    "from run.load_pretrained_weights import *\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from dataset.utils import nnUNet_resize\n",
    "import torch\n",
    "import random\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "config = {\n",
    "    'llm_folder':'/folder/store/llm_checkpoint',\n",
    "    'seg_folder':'/folder/store/seg_checkpoint',\n",
    "    'llm_chk':'llm_checkpoint_name',\n",
    "    'seg_chk':'seg_checkpoint_name',\n",
    "    'output_dir':'/folder/save/output/mask/and/report',\n",
    "    'eval_mode':'region_segtool'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trainer(config):\n",
    "    trainer, params = load_model_and_checkpoint_files_llm(config['llm_folder'], mixed_precision=True,\n",
    "                                                checkpoint_name=config['llm_chk'])\n",
    "    trainer.load_checkpoint_ram(params[0], False)\n",
    "    load_pretrained_weights(trainer.network, join(config['seg_folder'],config['seg_chk']+'.model'))\n",
    "\n",
    "    return trainer\n",
    "\n",
    "trainer = get_trainer(config)\n",
    "segmodel = SegModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emptying cuda cache\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LanguageModel(\n",
       "  (gpt_with_lm_head): GPT2LMHeadModel(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(50257, 1024)\n",
       "      (wpe): Embedding(1024, 1024)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-23): 24 x GPT2Block(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2PseudoAttention(\n",
       "            (c_attn): Conv1DWithTrainedWeights()\n",
       "            (c_proj): Conv1DWithTrainedWeights()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (uk): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (uv): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       "  )\n",
       "  (gpt): GPT2Model(\n",
       "    (wte): Embedding(50257, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2PseudoAttention(\n",
       "          (c_attn): Conv1DWithTrainedWeights()\n",
       "          (c_proj): Conv1DWithTrainedWeights()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (uk): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (uv): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       "  (wte): Embedding(50257, 1024)\n",
       "  (wpe): Embedding(1024, 1024)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (gpt2_blocks): ModuleList(\n",
       "    (0-23): 24 x ModuleList(\n",
       "      (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (1): GPT2PseudoAttention(\n",
       "        (c_attn): Conv1DWithTrainedWeights()\n",
       "        (c_proj): Conv1DWithTrainedWeights()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (uk): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (uv): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "print(\"emptying cuda cache\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "setup_seed(42)\n",
    "trainer.network.eval()\n",
    "trainer.llm_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = [\n",
    "    {\n",
    "      \"image\": \"path/to/image.nii.gz\",\n",
    "      \"modal\": \"T2FLAIR\"\n",
    "    }\n",
    "]\n",
    "\n",
    "hammer_anas = json.load(open('utils_file/hammer_anas.json','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_lists = [[j['image']] for j in test_file]\n",
    "list_of_ab_segs = [j['label'] if 'label' in j else None for j in test_file]\n",
    "list_of_ana_segs = [j['label2'] if 'label2' in j else None for j in test_file]\n",
    "list_of_reports = None if 'report' not in test_file[0] else [j['report'] for j in test_file]\n",
    "modals = [j['modal'] for j in test_file]\n",
    "\n",
    "list_of_ab_segs, list_of_ana_segs = segmodel.seg(list_of_lists, list_of_ab_segs, list_of_ana_segs, modals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before crop: (1, 155, 240, 240) after crop: (1, 146, 171, 136) spacing: [1. 1. 1.] \n",
      "\n",
      "no resampling necessary\n",
      "no resampling necessary\n",
      "before: {'spacing': array([1., 1., 1.]), 'spacing_transposed': array([1., 1., 1.]), 'data.shape (data is transposed)': (1, 146, 171, 136)} \n",
      "after:  {'spacing': array([1., 1., 1.]), 'data.shape (data is resampled)': (1, 146, 171, 136)} \n",
      "\n",
      "before crop: (1, 155, 240, 240) after crop: (1, 146, 171, 136) spacing: [1. 1. 1.] \n",
      "\n",
      "no resampling necessary\n",
      "no resampling necessary\n",
      "before: {'spacing': array([1., 1., 1.]), 'spacing_transposed': array([1., 1., 1.]), 'data.shape (data is transposed)': (1, 146, 171, 136)} \n",
      "after:  {'spacing': array([1., 1., 1.]), 'data.shape (data is resampled)': (1, 146, 171, 136)} \n",
      "\n",
      "debug: mirroring False mirror_axes (0, 1, 2)\n",
      "do mirror: False\n"
     ]
    }
   ],
   "source": [
    "pred_report = []\n",
    "for i, the_image_path in enumerate(list_of_lists):\n",
    "    the_ab_seg_path = list_of_ab_segs[i] if list_of_ab_segs is not None else None\n",
    "    the_ana_seg_path = list_of_ana_segs[i] if list_of_ana_segs is not None else None\n",
    "\n",
    "    if the_ab_seg_path is not None:\n",
    "        d, s_ab, dct = trainer.preprocess_patient(the_image_path, the_ab_seg_path, target_shape=None)\n",
    "    else:\n",
    "        s_ab = None\n",
    "    \n",
    "    if the_ana_seg_path is not None:\n",
    "        d, s_ana, dct = trainer.preprocess_patient(the_image_path, the_ana_seg_path, target_shape=None)\n",
    "    else:\n",
    "        s_ana = None\n",
    "    \n",
    "    modal = modals[i]\n",
    "\n",
    "    d = np.expand_dims(nnUNet_resize(d[0],trainer.patch_size,axis=0),axis=0)\n",
    "    s_ab = nnUNet_resize(s_ab[0], trainer.patch_size,is_seg=True,axis=0) if s_ab is not None else np.zeros(trainer.patch_size)\n",
    "    s_ab = np.expand_dims(s_ab, axis=0)\n",
    "    s_ana = nnUNet_resize(s_ana[0], trainer.patch_size,is_seg=True,axis=0) if s_ana is not None else np.zeros(trainer.patch_size)\n",
    "    s_ana = np.expand_dims(s_ana, axis=0)\n",
    "    s = np.concatenate((s_ana,s_ab),axis=0)\n",
    "\n",
    "    region_features, region_direction_names  = trainer.predict_preprocessed_data_return_region_report(\n",
    "        d, s, None, do_mirroring=False, mirror_axes=trainer.data_aug_params['mirror_axes'], use_sliding_window=True,\n",
    "        step_size=0.5, use_gaussian=True, all_in_gpu=False,\n",
    "        mixed_precision=True, modal=modal, eval_mode=config['eval_mode'])\n",
    "    \n",
    "    region_features = torch.tensor(np.array([item.cpu().detach().numpy() for item in region_features]), dtype=torch.float32).to(trainer.llm_model.device)\n",
    "    \n",
    "    output = trainer.llm_model.generate(\n",
    "                    region_features,\n",
    "                    max_length=300,\n",
    "                    num_beams=1,\n",
    "                    num_beam_groups=1,\n",
    "                    do_sample=False,\n",
    "                    num_return_sequences = 1,\n",
    "                    early_stopping=True\n",
    "            )\n",
    "    del region_features\n",
    "\n",
    "    generated_sents_for_selected_regions = trainer.tokenizer.batch_decode(\n",
    "            output, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "    )\n",
    "\n",
    "    ## the index of the sentence mask ##\n",
    "    if config['eval_mode'] == \"region_segtool\":\n",
    "        pred_global_report = generated_sents_for_selected_regions[-1]\n",
    "\n",
    "        pred_region_concat_report = []\n",
    "        for cur_idx, se in enumerate(generated_sents_for_selected_regions[:-1]):\n",
    "            # no anatomy is mentioned in the sentence\n",
    "            ana_flag = False\n",
    "            for cur_a in hammer_anas:\n",
    "                if cur_a in se.lower():\n",
    "                    ana_flag=True\n",
    "                    break\n",
    "            # anatomy is mentioned in the sentence\n",
    "            if not ana_flag:\n",
    "                sort_ana = sorted(region_direction_names[cur_idx][1],key =lambda x:-x[0])\n",
    "                most_pixel = sort_ana[0][0]\n",
    "                sort_ana = list(filter(lambda x:x[0]>=most_pixel, sort_ana))\n",
    "                ana_str = ' and '.join([item[1] for item in sort_ana])\n",
    "                se = se.strip()\n",
    "                se = se[:-1] + ' in '+ana_str+'.' if se[-1] == '.' or se[-1] == ',' else se + ' in '+ana_str+'.'\n",
    "            else:\n",
    "                if region_direction_names[cur_idx][0] == \"left\":\n",
    "                    se = se.replace('right','left')\n",
    "                elif region_direction_names[cur_idx][0] == \"right\":\n",
    "                    se = se.replace('left','right')\n",
    "            pred_region_concat_report.append(se)\n",
    "            \n",
    "        pred_region_concat_report = \" \".join(pred_region_concat_report)\n",
    "\n",
    "        left_sentence = \"\"\n",
    "\n",
    "        pred_split = pred_global_report.split('.')\n",
    "        pred_split_2 = []\n",
    "        for se in pred_split:\n",
    "            pred_split_2.extend(se.split(','))\n",
    "        pred_split = list(map(lambda x:x+'.',pred_split_2))\n",
    "        \n",
    "        if 'ventricle' not in pred_region_concat_report.lower() and 'ventricle' not in left_sentence.lower():\n",
    "            left_sentence = left_sentence+\" \".join([g for g in pred_split if 'ventricle' in g.lower()])            \n",
    "        if 'midline' not in pred_region_concat_report.lower() and 'midline' not in left_sentence.lower():\n",
    "            left_sentence = left_sentence+\" \"+\" \".join([g for g in pred_split if 'midline' in g.lower()])\n",
    "        if 'sulci' not in pred_region_concat_report.lower() and 'midline' not in left_sentence.lower():\n",
    "            left_sentence = left_sentence+\" \"+\" \".join([g for g in pred_split if 'sulci' in g.lower()])\n",
    "        \n",
    "        if 'midline' not in pred_region_concat_report.lower() and 'midline' not in left_sentence.lower():\n",
    "            left_sentence += \" No midline shift.\"\n",
    "            \n",
    "        pred_region_concat_report +=\" \"+left_sentence\n",
    "        \n",
    "        pred_report.append({'image':the_image_path,'pred_report':pred_region_concat_report,'ab_mask':the_ab_seg_path,'ana_mask':the_ana_seg_path})\n",
    "\n",
    "    elif config['eval_mode'] == \"given_mask\":\n",
    "        pred_region_concat_report = \" \".join(generated_sents_for_selected_regions)\n",
    "        pred_report.append({'image':the_image_path,'pred_report':pred_region_concat_report,'ab_mask':the_ab_seg_path,'ana_mask':the_ana_seg_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(pred_report)\n",
    "with open(os.path.join(config['output_dir'],'pred_report.json'), 'w') as f:\n",
    "    json.dump(pred_report, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ra_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
